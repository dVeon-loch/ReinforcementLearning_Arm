diff --git a/src/arm_bringup/scripts/arm_env.py b/src/arm_bringup/scripts/arm_env.py
index ff526b8..9fa058a 100644
--- a/src/arm_bringup/scripts/arm_env.py
+++ b/src/arm_bringup/scripts/arm_env.py
@@ -83,16 +83,19 @@ class AllJoints:
 
 class ArmEnvironment(gym.Env):
     def __init__(self, static_goal, slow_step=False, larger_radius=False):
+        self.arrived_counter = 0
+        self.static_gentests = False
         self.max_sim_time = 15
         if(larger_radius):
             self.goal_radius = 0.06 #sphere radius in ball.urdf is slightly bigger to improve visuals
         else:
-            self.goal_radius = 0.05
+            self.goal_radius = 0.05#0.05
         self.distance_reward_coeff = 5
         self.static_goal_pos = np.array([0.2,0.1,0.15])
         self.static_goal = static_goal
         self.slow_step = slow_step
         self.eef_pos = np.zeros(3)
+        self.goal_pos_list=np.array([[0.2,0.1,0.15],[-0.2,0.1,0.15],[-0.2,-0.1,0.15],[0.3,0.05,0.2],[0.15195154, 0.12716517, 0.10791227],[-0.2443342 , -0.26623122,  0.01693387],[ 0.20605877, -0.16932273,  0.18279588],[-0.25533529, -0.11042604,  0.14230928],[ 0.3006918 , -0.00099998,  0.16692274],[-0.07770072,  0.0790152 ,  0.29283088],[ 0.18422977, -0.16340892,  0.27168277],[ 0.28851676, -0.0598984 ,  0.20165042],[-0.09136536, -0.00854591,  0.20377172]])
 
         self.zero = np.array([0,0,0,0])
         #rospy.init_node('joint_position_node')
@@ -107,12 +110,15 @@ class ArmEnvironment(gym.Env):
         if(self.static_goal):
             self.goal_pos = self.static_goal_pos
         else:
-            while(True):
-                x_y = np.random.uniform(low = -0.4, high = 0.4, size = 2)
-                z = np.random.uniform(low = 0, high = 0.4, size = 1)
-                self.goal_pos = np.concatenate([x_y,z],axis=0)
-                if(np.linalg.norm(self.goal_pos)<0.4):
-                    break
+            index = np.random.randint(low=0,high=len(self.goal_pos_list)-1)
+            self.goal_pos = self.goal_pos_list[index]
+            print("Goal pos: ", self.goal_pos)
+            # while(True):
+            #     x_y = np.random.uniform(low = -0.4, high = 0.4, size = 2)
+            #     z = np.random.uniform(low = 0, high = 0.4, size = 1)
+            #     self.goal_pos = np.concatenate([x_y,z],axis=0)
+            #     if(np.linalg.norm(self.goal_pos)<0.4):
+            #         break
         rospy.loginfo("Goal position defined")
         self.pause_physics_proxy = rospy.ServiceProxy('/gazebo/pause_physics',Empty, persistent=False)
         self.unpause_physics_proxy = rospy.ServiceProxy('/gazebo/unpause_physics',Empty, persistent=False)
@@ -203,24 +209,78 @@ class ArmEnvironment(gym.Env):
         #     self.reset_proxy()
         # except (rospy.ServiceException) as e:
         #     print("gazebo/reset_simulation service call failed")
+    
+    def step(self, action):
+        action = np.array(action)
 
+        self.joint_pos = np.clip(self.joint_state + action,a_min=self.joint_pos_low,a_max=self.joint_pos_high)
+        self.all_joints.move(self.joint_pos)
+        if(self.slow_step):
+            rospy.sleep(0.35)
+        else:
+            rospy.sleep(0.01)
+        (joint_angles, time_runout, arrived) = self.get_state()
+        reward = self.get_reward(time_runout, arrived)
+        if(time_runout or arrived):
+            done = True
+        else:
+            done = False
+        state = np.concatenate([joint_angles,self.goal_pos])
+        state = np.asarray(state, dtype = np.float32)
 
-    def set_model_state(self, set_state_request):
-        rospy.wait_for_service('/gazebo/set_model_state')
+        return state, reward, done, {}
+   
+    def reset(self):
+
+        rospy.wait_for_service('/gazebo/delete_model')
+        self.del_model('arm')
+        #self.del_model('simple_ball')
+        rospy.sleep(0.5)
+        rospy.wait_for_service('gazebo/reset_simulation')
         try:
-            self.state_proxy(set_state_request)
-            return True
-        except rospy.ServiceException as e:
-            print('/gazebo/set_model_state service call failed')
-            return False
+            self.reset_proxy()
+        except (rospy.ServiceException) as e:
+            print("gazebo/reset_simulation service call failed")
+        self.spawn_model(self.arm_model)
 
-    def clock_subscriber_callback(self, clock):
-        self.current_time = clock.clock.secs
+        #rospy.sleep(0.5)
+        #self.spawn_model(self.arm_model)
+        #rospy.sleep(1)
+        rospy.wait_for_service('arm/controller_manager/load_controller')
+        try:
+            self.load_controller_proxy(self.joint_state_controller_load)
+        except (rospy.ServiceException) as e:
+            print('arm/controller_manager/load_controller service call failed')
+        
+        rospy.wait_for_service('arm/controller_manager/load_controller')
+        try:
+            self.load_controller_proxy(self.arm_controller_load)
+        except (rospy.ServiceException) as e:
+            print('arm/controller_manager/load_controller service call failed')
+
+        rospy.wait_for_service('arm/controller_manager/switch_controller')
+        try:
+            self.switch_controller_proxy(self.switch_controller)
+        except (rospy.ServiceException) as e:
+            print('arm/controller_manager/switch_controller service call failed')
 
-    def normalize_joint_state(self, joint_pos):
-        # TODO implement normalization
-        return joint_pos
 
+        self.set_new_goal()
+        rospy.sleep(1)
+       # rospy.sleep(3)
+        # self.last_goal_distance = self.get_goal_distance()
+        done = False
+        joint_angles, time_runout, arrived = self.get_state()
+        #self.last_joint = self.joint_state
+        #self.last_pos = pos
+        # diff_joint = np.zeros(self.num_joints)
+        state = np.concatenate([joint_angles,self.goal_pos])#.reshape(1, -1)
+        state = np.asarray(state, dtype = np.float32)
+        self.joint_state = np.zeros(4)
+        self.joint_pos = np.zeros(4)
+
+        return state
+    
     def get_state(self):
         self.hit_floor = False
         joint_angles = self.joint_state
@@ -236,14 +296,37 @@ class ArmEnvironment(gym.Env):
             arrived=False
         if(self.hit_floor or current_sim_time>=self.max_sim_time):
             time_runout = True
-            print("ran out of time or hit floor")
+            #print("ran out of time or hit floor")
         else:
             time_runout=False
         return joint_angles, time_runout, arrived
+    
+    def get_reward(self, time_runout, arrive):
+        reward = -1*self.distance_reward_coeff*self.get_goal_distance()
+        if(self.hit_floor):
+            reward = reward - 50.0
+        if(time_runout and not arrive):
+            reward = reward - 25.0
+        if(arrive):
+            self.arrived_counter += 1
+            print("Arrived at goal")
+            reward += 25.0
+        return reward
+    
+    def set_model_state(self, set_state_request):
+        rospy.wait_for_service('/gazebo/set_model_state')
+        try:
+            self.state_proxy(set_state_request)
+            return True
+        except rospy.ServiceException as e:
+            print('/gazebo/set_model_state service call failed')
+            return False
+
+    def clock_subscriber_callback(self, clock):
+        self.current_time = clock.clock.secs
 
     def joint_state_subscriber_callback(self, joint_state):
         self.joint_state = np.array(joint_state.actual.positions)
-        
 
     def pause_physics(self):
         rospy.wait_for_service('/gazebo/pause_physics')
@@ -276,32 +359,25 @@ class ArmEnvironment(gym.Env):
 
     def set_new_goal(self):
         rospy.loginfo("Defining a goal position...")
+        
         if(self.static_goal):
             self.goal_pos = self.static_goal_pos
         else:
             # x_y = np.random.uniform(low = -0.4, high = 0.4, size = 2)
             # z = np.random.uniform(low = 0, high = 0.4, size = 1)
             # self.goal_pos = np.concatenate([x_y,z],axis=0)
-            while(True):
-                x_y = np.random.uniform(low = -0.4, high = 0.4, size = 2)
-                z = np.random.uniform(low = 0, high = 0.4, size = 1)
-                self.goal_pos = np.concatenate([x_y,z],axis=0) #np.array([-0.13242582 , 0.29086919 , 0.20275278])
-                if(np.linalg.norm(self.goal_pos)<0.4 and np.linalg.norm(self.goal_pos)>0.1):
-                    break
-        #rospy.loginfo("Goal position defined")
-        #rospy.loginfo("Goal position: "+str(self.goal_pos))
-        # self.sphere_urdf = open(sphere_dir, "r").read()
-        # self.sphere = SpawnModelRequest()
-        # self.sphere.model_name = 'simple_ball'  # the same with sdf name
-        # self.sphere.model_xml = self.sphere_urdf
-        # self.sphere.robot_namespace = 'arm'
-        # self.sphere_initial_pose = Pose()
-        # self.sphere_initial_pose.position.x = self.goal_pos[0]
-        # self.sphere_initial_pose.position.y = self.goal_pos[1]
-        # self.sphere_initial_pose.position.z = self.goal_pos[2]
-        # self.sphere.initial_pose = self.sphere_initial_pose 
-        # self.sphere.reference_frame = 'world'
-        # self.spawn_model(self.sphere)
+            index = np.random.randint(low=0,high=len(self.goal_pos_list)-1)
+            self.goal_pos = self.goal_pos_list[index]
+                # x_y = np.random.uniform(low = -0.4, high = 0.4, size = 2)
+                # z = np.random.uniform(low = 0, high = 0.4, size = 1)
+                # self.goal_pos = np.concatenate([x_y,z],axis=0) #np.array([-0.13242582 , 0.29086919 , 0.20275278])
+                # if(np.linalg.norm(self.goal_pos)<0.4 and np.linalg.norm(self.goal_pos)>0.1):
+                #     break
+        if(self.static_gentests):
+            x_y = np.random.uniform(low = -0.1, high = 0.1, size = 2)
+            z = np.random.uniform(low = -0.05, high = 0.1, size = 1)
+            perturb = np.concatenate([x_y,z],axis=0)
+            self.goal_pos = self.goal_pos + perturb
         sphere_pose = Pose()
         sphere_pose.position.x = self.goal_pos[0]
         sphere_pose.position.y = self.goal_pos[1]
@@ -317,87 +393,11 @@ class ArmEnvironment(gym.Env):
             print("/gazebo/failed to build the target")
         self.unpause_physics()
 
-    def get_reward(self, time_runout, arrive):
-        reward = -1*self.distance_reward_coeff*self.get_goal_distance()
-        if(self.hit_floor):
-            reward = reward - 50.0
-        if(time_runout and not arrive):
-            reward = reward - 25.0
-        if(arrive):
-            print("Arrived at goal")
-            reward += 25.0
-        return reward
-        
-    def reset(self):
-
-        rospy.wait_for_service('/gazebo/delete_model')
-        self.del_model('arm')
-        #self.del_model('simple_ball')
-        rospy.sleep(0.5)
-        rospy.wait_for_service('gazebo/reset_simulation')
-        try:
-            self.reset_proxy()
-        except (rospy.ServiceException) as e:
-            print("gazebo/reset_simulation service call failed")
-        self.spawn_model(self.arm_model)
-
-        #rospy.sleep(0.5)
-        #self.spawn_model(self.arm_model)
-        #rospy.sleep(1)
-        rospy.wait_for_service('arm/controller_manager/load_controller')
-        try:
-            self.load_controller_proxy(self.joint_state_controller_load)
-        except (rospy.ServiceException) as e:
-            print('arm/controller_manager/load_controller service call failed')
-        
-        rospy.wait_for_service('arm/controller_manager/load_controller')
-        try:
-            self.load_controller_proxy(self.arm_controller_load)
-        except (rospy.ServiceException) as e:
-            print('arm/controller_manager/load_controller service call failed')
-
-        rospy.wait_for_service('arm/controller_manager/switch_controller')
-        try:
-            self.switch_controller_proxy(self.switch_controller)
-        except (rospy.ServiceException) as e:
-            print('arm/controller_manager/switch_controller service call failed')
-
-
-        self.set_new_goal()
-        rospy.sleep(1)
-       # rospy.sleep(3)
-        # self.last_goal_distance = self.get_goal_distance()
-        done = False
-        joint_angles, time_runout, arrived = self.get_state()
-        #self.last_joint = self.joint_state
-        #self.last_pos = pos
-        # diff_joint = np.zeros(self.num_joints)
-        state = np.concatenate([joint_angles,self.goal_pos])#.reshape(1, -1)
-        state = np.asarray(state, dtype = np.float32)
-        self.joint_state = np.zeros(4)
-        self.joint_pos = np.zeros(4)
-
-        return state
     def seed(self,seed):
         pass
         
-    def step(self, action):
-        action = np.array(action)
-
-        self.joint_pos = np.clip(self.joint_state + action,a_min=self.joint_pos_low,a_max=self.joint_pos_high)
-        self.all_joints.move(self.joint_pos)
-        if(self.slow_step):
-            rospy.sleep(0.35)
-        else:
-            rospy.sleep(0.01)
-        (joint_angles, time_runout, arrived) = self.get_state()
-        reward = self.get_reward(time_runout, arrived)
-        if(time_runout or arrived):
-            done = True
-        else:
-            done = False
-        state = np.concatenate([joint_angles,self.goal_pos])
-        state = np.asarray(state, dtype = np.float32)
-
-        return state, reward, done, {}
+    def normalize_joint_state(self, joint_pos):
+        # TODO implement normalization
+        return joint_pos
+    
 
diff --git a/src/arm_bringup/scripts/config_test.py b/src/arm_bringup/scripts/config_test.py
index 1a96fd0..43e666b 100644
--- a/src/arm_bringup/scripts/config_test.py
+++ b/src/arm_bringup/scripts/config_test.py
@@ -21,9 +21,6 @@ while(True):
     action = np.array(floatarr)
     env.step(floatarr)
     print("Goal Distance: ",env.get_goal_distance())
-    reset = eval(input("Reset goal?"))
-    if(reset):
-        env.set_new_goal()
 
 
 # alljoints.move(pos)
diff --git a/src/arm_bringup/scripts/ppo_train_test.py b/src/arm_bringup/scripts/ppo_train_test.py
index ab785e1..f73f68e 100644
--- a/src/arm_bringup/scripts/ppo_train_test.py
+++ b/src/arm_bringup/scripts/ppo_train_test.py
@@ -7,8 +7,10 @@ import numpy as np
 
 normalise_obs = False
 static_goal = True
-testing = False
+testing = True
 slow_step = True
+num_tests = 50
+
 
 rospy.init_node("RL_agent")
 parser = OnPolicyTrainer.get_argument()
@@ -32,6 +34,8 @@ parser.set_defaults(normalise_obs=normalise_obs)
 parser.set_defaults(save_model_interval=100)
 parser.set_defaults(save_summary_interval=100)
 parser.set_defaults(test_interval=500)
+if(testing):
+    parser.set_defaults(test_episodes=num_tests)
 
 #parser.set_defaults(horizon=1024)
 #parser.set_defaults(batch_size=512)
@@ -68,6 +72,7 @@ policy = PPO(
 trainer = OnPolicyTrainer(policy, env, args, test_env=test_env)
 
 if(testing):
-    trainer.evaluate_policy()
+    trainer.evaluate_policy(num_tests)
+    print("Arrived "+str(test_env.arrived_counter)+" times out of "+str(num_tests)+" tests.")
 else:
     trainer()
\ No newline at end of file
diff --git a/src/arm_bringup/scripts/sac_train_test.py b/src/arm_bringup/scripts/sac_train_test.py
index 0799b5c..997f6a1 100644
--- a/src/arm_bringup/scripts/sac_train_test.py
+++ b/src/arm_bringup/scripts/sac_train_test.py
@@ -5,9 +5,10 @@ import rospy
 import numpy as np
 
 normalise_obs = False
-static_goal = True
+static_goal = False
 testing = False
 slow_step = True
+num_tests = 50
 
 parser = Trainer.get_argument()
 parser = SAC.get_argument(parser)
@@ -30,6 +31,8 @@ parser.set_defaults(normalise_obs=normalise_obs)
 parser.set_defaults(save_model_interval=100)
 parser.set_defaults(save_summary_interval=100)
 parser.set_defaults(test_interval=500)
+if(testing):
+    parser.set_defaults(test_episodes=num_tests)
 
 
 parser.set_defaults(gpu=-1)
@@ -58,6 +61,7 @@ trainer = Trainer(policy, env, args, test_env=test_env)
 #trainer.evaluate_policy_continuously()
 
 if(testing):
-    trainer.evaluate_policy_continuously()
+    trainer.evaluate_policy(num_tests)
+    print("Arrived "+str(test_env.arrived_counter)+" times out of "+str(num_tests)+" tests.")
 else:
     trainer()
\ No newline at end of file
diff --git a/src/arm_bringup/scripts/td3_train_test.py b/src/arm_bringup/scripts/td3_train_test.py
index 4a52c5c..72c3187 100644
--- a/src/arm_bringup/scripts/td3_train_test.py
+++ b/src/arm_bringup/scripts/td3_train_test.py
@@ -5,9 +5,11 @@ import rospy
 import numpy as np
 
 normalise_obs = False
-static_goal = False
-testing = False
+static_goal = True
+testing = True
 slow_step = True
+num_tests = 50
+
 # conda activate RL_arm_noetic && source devel/setup.bash && tensorboard --logdir /home/devon/RL_Arm_noetic/results
 
 parser = Trainer.get_argument()
@@ -34,6 +36,9 @@ parser.set_defaults(save_summary_interval=100)
 parser.set_defaults(test_interval=500)
 parser.set_defaults(max_steps=100000000)
 parser.set_defaults(gpu=-1)
+if(testing):
+    parser.set_defaults(test_episodes=num_tests)
+
 
 #parser.set_defaults(show_progress=True)
 args = parser.parse_args()
@@ -54,6 +59,7 @@ policy = TD3(
 trainer = Trainer(policy, env, args, test_env=test_env)
 
 if(testing):
-    trainer.evaluate_policy()
+    trainer.evaluate_policy(num_tests)
+    print("Arrived "+str(test_env.arrived_counter)+" times out of "+str(num_tests)+" tests.")
 else:
     trainer()
\ No newline at end of file
